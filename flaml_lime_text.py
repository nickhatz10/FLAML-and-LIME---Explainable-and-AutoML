# -*- coding: utf-8 -*-
"""FLAML_LIME_TEXT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EF7r1VCxqIj4OKrPJkazZFGKhoPg4znM
"""

pip install lime

pip install flaml

# mount my google drive to this notebook so I can access my files
from google.colab import drive
drive.mount('/content/drive')

# gets positive tweets and negative tweets text files
positive_tweets = '/content/drive/My Drive/DS 440/positive_tweet.txt'

negative_tweets = '/content/drive/My Drive/DS 440/negative_tweet.txt'

# various import statements we will need
import re
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

# two packages we will use
nltk.download('wordnet')

nltk.download('stopwords')

# cleaning function to clean up the tweets
def cleaning(review, remove_stopwords=True):

    #removes urls which wont be of any use
    removed_urls = re.sub(r'https?://\S+', "", review)
    
    # removing any non alphabetical characters
    review_text = re.sub("[^a-zA-Z]"," ", removed_urls)

    # turns all characters in the string to lowercase and splits them by spaces
    words = review_text.lower().split(" ")

    # if remove_stopwords is True, then we remove any words in the list that are stop words
    if remove_stopwords:
        stops = set(stopwords.words("english"))
        words = [word for word in words if word not in stops]

    # stems each word in the word list
    stemmer = nltk.stem.SnowballStemmer('english')
    stemmed_words = [(stemmer.stem(word)) for word in words]
    
    
    # removes any words in the list that are just blank spaces
    cleaned_words = [word for word in stemmed_words if len(word) > 0]

    # returns final list of cleaned words
    return(cleaned_words)

# opens the text files and then applies the cleaning function to them
with open(positive_tweets) as f:
    lines = f.readlines()
    cleaned_data = []
    for line in lines:
      cleaned_line = cleaning(line, remove_stopwords=True)
      cleaned_data.append(' '.join(cleaned_line))
f.close()

df_pos = pd.DataFrame(cleaned_data, columns=['cleaned_tweets'])

with open(negative_tweets) as f:
    lines = f.readlines()
    cleaned_data = []
    for line in lines:
      cleaned_line = cleaning(line, remove_stopwords=True)
      cleaned_data.append(' '.join(cleaned_line))

f.close()

# creates a new negative dataframe with column named cleaned tweets and then negative tweets are labeled 0
df_neg = pd.DataFrame(cleaned_data, columns=['cleaned_tweets'])
df_neg['sentiment'] = 0

# creates a new positive dataframe with positive tweets being labeled as 1
df_pos['sentiment'] = 1

# combines the negative and positive dataframes
dataset = pd.concat([df_neg, df_pos])

# resets the index of the new dataframe
dataset = dataset.reset_index(drop=True)

# train test split and then vectorize the training and testing data
from sklearn.model_selection import train_test_split

labels = dataset['sentiment']
tweets = dataset['cleaned_tweets']

from sklearn.feature_extraction.text import TfidfVectorizer 

X_train, X_test, y_train, y_test = train_test_split(tweets, labels, shuffle = True, test_size=0.2, random_state=30)

tfidf_vectorizer = TfidfVectorizer(use_idf=True) 

X_train_tf_idf = tfidf_vectorizer.fit_transform(X_train).toarray()
X_test_tf_idf = tfidf_vectorizer.transform(X_test)

# flaml to find the best text classification classifier
from flaml import AutoML
automl = AutoML()
automl_settings = {"metric": 'accuracy', "task": 'classification', 'time_budget' : 120}

automl.fit(X_train_tf_idf, y_train, **automl_settings)

# makes pipeline of ml classifier and feature vector of tweets
from sklearn.pipeline import make_pipeline
pipeline = make_pipeline(tfidf_vectorizer, automl)

# CLASS FUNCTION FOR TEXT CLASSIFICATION
# allows us to easily use FLAML and LIME together for text classification problems

class flaml_lime_text_classification:

  # initializes variables
  def __init__(self, ml_pipeline, raw_test_data, class_names, labels):
    self.ml_pipeline = ml_pipeline
    self.raw_test_data = raw_test_data
    self.labels = labels
    self.class_names = class_names

  # creates the LIME explainer
  def flaml_lime_text_explainer(self, **kwargs):
    import lime
    from lime import lime_text
    explainer = lime_text.LimeTextExplainer(class_names = self.class_names, **kwargs)
    return explainer

  # outputs the HTML image so we can see an explanation of how well FLAML distinguished between the various classes
  def flaml_lime_show(self, explainer, row_index = 0, **kwargs):
    import lime
    from lime import lime_tabular
    explainer = explainer
    print("True Label: {}\n".format(self.class_names[self.labels.values[row_index]]))
    exp = explainer.explain_instance(text_instance=self.raw_test_data.values[row_index], classifier_fn = self.ml_pipeline.predict_proba, **kwargs)

    return exp.show_in_notebook()

# calls our class object and its functions to make sure they work correctly
# shows that we were able to create a compatible and easy-to-use LIME and FLAML interaction function

flaml_lime_text = flaml_lime_text_classification(raw_test_data = X_test, labels = y_test, class_names = ['negative', 'positive'], ml_pipeline = pipeline)

explainer = flaml_lime_text.flaml_lime_text_explainer()

flaml_lime_text.flaml_lime_show(explainer, row_index = 35)